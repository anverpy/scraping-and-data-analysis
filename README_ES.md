
# Limpieza de Datos tras Web Scraping ‚Äì Categor√≠a Machine Learning & Data Analytics

## üì¢ Descargo de Responsabilidad
Este repositorio contiene datos que fueron obtenidos mediante web scraping desde un sitio web de acceso p√∫blico. El prop√≥sito de este proyecto es puramente educativo y no comercial. Est√° destinado a explorar t√©cnicas de an√°lisis de datos, visualizaci√≥n y machine learning con fines acad√©micos y de desarrollo personal.

No reclamamos la propiedad de ning√∫n contenido extra√≠do del sitio web, y ning√∫n dato ha sido utilizado para su redistribuci√≥n, reventa o con intenci√≥n de infringir derechos de propiedad intelectual. Se han tomado todas las medidas posibles para asegurar un uso √©tico de la informaci√≥n p√∫blicamente disponible.

Si eres representante del sitio original y tienes alguna inquietud, no dudes en contactarme para revisar o retirar el contenido seg√∫n corresponda.

---

### üìå Este documento resume el proceso de limpieza aplicado a los datos extra√≠dos de los resultados web de gigs de:</br> [*Machine Learning*](https://github.com/anverpy/scraping-and-data-analysis/blob/main/ML-gigs.csv) y [*Data Analytics*](https://github.com/anverpy/scraping-and-data-analysis/blob/main/DA-gigs.csv). <br> El enfoque est√° en resolver los problemas estructurales del CSV causados por comas embebidas en los t√≠tulos.</br>

### En el archivo [og_DA_csv.zip](https://github.com/anverpy/scraping-and-data-analysis/blob/main/og_DA_csv.zip), puedes encontrar los datos crudos de los gigs de Data Analytics. </br>

### No pude conservar el crudo de Machine Learning por un error t√©cnico. </br> (Dej√© de usar shift + del üòÖ).

---

### üîç Campos de Datos Esperados
Campos esperados para cada gig:
- `title`
- `seller`
- `rating`
- `reviews`
- `price`
- `seller_level`
- `video_consultation`

<br><br>
---
# üßπ 1. Limpieza de Datos

- ## 1.1 Algunos t√≠tulos de gigs conten√≠an comas sin el uso adecuado de comillas, lo que resultaba en filas con m√°s de 7 columnas. Ejemplo:

```csv
 I will create Machine Learning, Deep Learning, and NLP,Andrew Veasman,4.9,152,"1499,90","Level 2",True
```

## Si no se respetan las comillas, esto se interpreta como m√°s de 7 columnas, rompiendo la estructura del CSV.


- ## 1.2 Hay espacios en blanco en los campos de puntuaci√≥n, rese√±as y nivel porque hay m√°s datos de gigs que de vendedores, y porque algunos vendedores a√∫n no tienen un nivel asignado.
## ¬øQu√© significa esto?
### Los gigs reciben calificaciones y rese√±as por gig, no por vendedor.<br>Algunos vendedores tienen dos o m√°s gigs publicados (esto se ver√° m√°s adelante en la visualizaci√≥n), por lo tanto hay gigs sin calificaciones <br>ni rese√±as. (Un ejemplo claro es Johannes M.)
<br><br>
---

# üõ†Ô∏è 2. Estrategia de Soluci√≥n
## Se implement√≥ un enfoque de parsing inverso:
### 1. Para cada fila, se toman los √∫ltimos 6 elementos (campos fijos conocidos).
### 2. Se considera todo lo anterior como el `title`.
### 3. Se rellenan los vac√≠os en rating/reviews/seller_level con valores como unranked/unreviewed/unleveled

<br>

# üîß 3. Resumen de Scripts

### - **[`title_extractor.py`](https://github.com/anverpy/scraping-and-data-analysis/blob/main/title_extractor.py)**: Extrae solo la columna de t√≠tulo desde filas malformadas.
### - **[`rid_bad_titles.py`](https://github.com/anverpy/scraping-and-data-analysis/blob/main/rid_bad_titles.py)**: Conserva solo los √∫ltimos 6 campos, descartando t√≠tulos corruptos.
### - En este punto elimin√© todas las "," en Excel usando `CTRL + B`, pero puedes mejorarlo a√±adiendo esta funci√≥n a cualquiera de los scripts.
### - **[`merge_titles_untitleds.py`](https://github.com/anverpy/scraping-and-data-analysis/blob/main/merge_titles_untitleds.py)**: Une los t√≠tulos limpios con los datos corregidos.
### - **[`merge_all.py`](https://github.com/anverpy/scraping-and-data-analysis/blob/main/merge_all.py)**: Concatena dos CSV completos desde distintas sesiones de scraping. Aseg√∫rate de que los CSV tengan los mismos nombres de cabecera.
### - **[`fill_gaps.py`](https://github.com/anverpy/scraping-and-data-analysis/blob/main/fill_gaps.py)**: Llena huecos en rating/reviews/seller_level con unranked/unreviewed/unleveled.
## Una vez hecha la limpieza de datos, se contin√∫a al siguiente paso.

<br>

# üìä 4. Visualizaci√≥n
## En el repositorio encontrar√°s el archivo [`visualization.png`](https://github.com/anverpy/scraping-and-data-analysis/blob/main/visualization.png), que contiene las visualizaciones finales renderizadas.<br> Puedes descargar el archivo y analizarlo por tu cuenta o seg√∫n lo necesites.

<br>

# ‚úÖ Resultado
## Este proceso permiti√≥ recuperar datos limpios y estructurados desde CSVs corruptos ‚Äî un reto com√∫n en entornos reales de web scraping y an√°lisis de datos. <br> La l√≥gica de ingenier√≠a inversa estructural demostr√≥ ser precisa, robusta y reutilizable.
